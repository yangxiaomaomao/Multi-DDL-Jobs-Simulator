{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_size(batch_size, hidden_size, seq_len, FP, TP):\n",
    "    mega = 1024 ** 2\n",
    "    return batch_size * seq_len * hidden_size * FP / TP / mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_size(vocab_size, hidden_size, FP, TP):\n",
    "    mega = 1024 ** 2\n",
    "    return vocab_size * hidden_size * FP / TP / mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_peer_size(batch_size, seq_len, hidden_size, FP, layer_per_stage):\n",
    "    mega = 1024 ** 2\n",
    "    return 2 * batch_size * seq_len * hidden_size * layer_per_stage * FP / mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flops_per_layer(hidden_size, seq_len, batch_size):\n",
    "    return 4 * seq_len ** 2 * hidden_size * batch_size + 10 * seq_len * hidden_size ** 2 * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT3 large\n",
    "B = 4 # batch size\n",
    "H = 1024 # hidden size\n",
    "S = 2048 # sequence length\n",
    "FP = 2 # 2 bytes per float\n",
    "V = 50256 # vocabulary size\n",
    "L = 24 # number of layers\n",
    "\n",
    "# cluster config\n",
    "BW = 10000 # bandwidth MBps\n",
    "TOP_FLOPS = 312 * 10 ** 12 # 312 TFLOP for A100 FP16\n",
    "util_reduce = 0.6 # 60% utilization\n",
    "FLOPS = TOP_FLOPS * util_reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_info(TP,PP):\n",
    "    layers_per_stage = L / PP\n",
    "\n",
    "    print(\"Activation time (ms)\")\n",
    "    activation_time = get_activation_size(B, H, S, FP, TP) / BW * 1000\n",
    "    print(activation_time)\n",
    " \n",
    "    print(\"Embedding time (ms)\")\n",
    "    embedding_time = get_embedding_size(V, H, FP, TP)/BW * 1000\n",
    "    print(embedding_time)\n",
    "    \n",
    "    print(\"TP time (ms)\")\n",
    "    tp_peer_size = get_tp_peer_size(B, S, H, FP, layers_per_stage)/BW * 1000\n",
    "    print(tp_peer_size)\n",
    "\n",
    "    print(\"Per layer Total time (ms)\")\n",
    "    total_time = get_flops_per_layer(H, S, B) / FLOPS * 1000\n",
    "    print(total_time)\n",
    "    \n",
    "    print(\"Per stage comp time (ms)\")\n",
    "    comp_time = total_time * L / PP\n",
    "    print(comp_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation time (ms)\n",
      "0.8\n",
      "Embedding time (ms)\n",
      "4.9078124999999995\n",
      "TP time (ms)\n",
      "9.6\n",
      "Per layer Total time (ms)\n",
      "0.8259552492307692\n",
      "Per stage comp time (ms)\n",
      "2.4778657476923076\n"
     ]
    }
   ],
   "source": [
    "# parallel config\n",
    "PP = 8 # number of wparallel pipelines\n",
    "TP = 2 # tensor parallelism\n",
    "show_info(TP,PP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
